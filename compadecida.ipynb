{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92a82ec3-e840-4dc5-ba70-c3380592d742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# treinar um modelo usando como base o livro do auto da compadecida\n",
    "with open('data/auto_da_compadecida.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c915e942-25f4-41af-a1fe-ebd2508979ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do datatset(caracteres):  74462\n"
     ]
    }
   ],
   "source": [
    "print(\"Tamanho do datatset(caracteres): \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5624ed27-305b-453e-81ae-2a3437a2b795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              O AUTO DA COMPADECIDA  \n",
      "                                                                        De Ariano Suassuna  \n",
      " \n",
      "JOÃO GRILO – Então Chico o homem vem mesmo? Estou tão desconfiado, tu é tão sem confiança!  \n",
      "CHICÓ – eu sem confiança? Que isso João está me desconhecendo? Pois eu juro como ele vem. Quer benzer \n",
      "o cachorro da mulher pra ver se ele não morre. A dificuldade não é ele não vir, mas é o padre benzer o \n",
      "bicho.  \n",
      "JOÃO GRILO – Não  vai benzer por que?Que é que tem de mais em se benzer um cachorro?  \n",
      "CHICÓ – Bom, eu digo assim porque eu sei como esse povo é cheio de coisas. Eu mesmo já tive cavalo \n",
      "bento.  \n",
      "JOÃO GRILO – Que é isso Chico? Já estou ficando por aqui com suas histórias e quando se pede uma \n",
      "explicação vem sempre com a mesma ladainha: “Não sei só sei que foi assim”  \n",
      "CHICÓ – Mas se eu tive mesmo um cavalo meu filho, o que é que eu vou fazer? Vou mentir.  Dizer \n",
      "que não tive?  \n",
      "JOÃO GRILO – Você vem sempre com uma histór\n"
     ]
    }
   ],
   "source": [
    "#Printa as primeiros 1000 caracteres\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c13d56a-6295-437d-85e8-e7bc19086e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Hiperparametros\n",
    "batch_size = 64 \n",
    "block_size = 256 \n",
    "max_iters = 1000\n",
    "eval_interval = 100\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c51a1c8-5f3f-42ca-a80f-7fc9ac975dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# caracteres unicos do auto da compadecida\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# mapeamento dos caracteres em inteiros e vice-versa\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # codificador\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decodificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ee6afb5-4dae-42ec-9ef9-b6ca58709206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir dataset em Treino e Validação\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.85*len(data)) # os primeiros 85% vão ser treino, e o resto validação\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8a0217a-bb0f-457f-bd5f-93b374f498ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamento dos dados\n",
    "def get_batch(split):\n",
    "    # Gera um pequeno lote de dados com input x e alvo y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba2af7fa-d5aa-4c54-b121-ffa296b16d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcao para estimar a perda\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d65ecb7-05c7-48e1-9d08-a35da20fd7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" Uma cabeça de autoatenção \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # entrada de tamanho (batch, etapa temporal, canais)\n",
    "        # saída de tamanho (batch, etapa temporal, tamanho da cabeça)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # calcular os scores de atenção (\"afinidades\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # realizar a agregação ponderada dos valores\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dc4726b-ce59-4172-8700-6d0e1725de5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Múltiplas cabeças de autoatenção em paralelo \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d015917-61b7-402b-940b-2a4e18a4ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" uma camada linear simples seguida de uma não linearidade \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96565df4-812f-4ca9-85b2-3506d7b035ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Bloco Transformer: comunicação seguida de computação \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: dimensão da incorporação, n_head: o número de cabeças desejadas\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a582572-9bb4-41dc-81ea-cd8f82574181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.807385 M parâmetros\n"
     ]
    }
   ],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # cada token lê diretamente os logits para o próximo token de uma tabela de busca\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # camada de normalização final\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # inicialização melhorada\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx e targets são ambos tensores (B,T) de inteiros\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx é um array (B, T) de índices no contexto atual\n",
    "        for _ in range(max_new_tokens):\n",
    "            # recorta idx para os últimos block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # obtém as previsões\n",
    "            logits, loss = self(idx_cond)\n",
    "            # foca apenas no último passo de tempo\n",
    "            logits = logits[:, -1, :] # se torna (B, C)\n",
    "            # aplica softmax para obter probabilidades\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # amostra da distribuição\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # anexa o índice amostrado à sequência em execução\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# imprime o número de parâmetros no modelo\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parâmetros')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13fc25a2-c540-4eb0-ab7b-5ccc0d16167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar um PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22d6fc18-634e-44bc-a71f-2f29f9e9e17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteração 0: perda no treino 0.1327, perda na validação 2.5610\n",
      "Iteração 100: perda no treino 0.0981, perda na validação 2.7461\n",
      "Iteração 200: perda no treino 0.0845, perda na validação 2.8757\n",
      "Iteração 300: perda no treino 0.0764, perda na validação 2.9906\n",
      "Iteração 400: perda no treino 0.0729, perda na validação 3.0528\n",
      "Iteração 500: perda no treino 0.0678, perda na validação 3.1451\n",
      "Iteração 600: perda no treino 0.0653, perda na validação 3.2379\n",
      "Iteração 700: perda no treino 0.0644, perda na validação 3.2673\n",
      "Iteração 800: perda no treino 0.0609, perda na validação 3.3179\n",
      "Iteração 900: perda no treino 0.0596, perda na validação 3.3329\n",
      "Iteração 999: perda no treino 0.0589, perda na validação 3.3334\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    # de vez em quando, avalie a perda nos conjuntos de treino e validação\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Iteração {iter}: perda no treino {losses['train']:.4f}, perda na validação {losses['val']:.4f}\")\n",
    "\n",
    "    # amostra um lote de dados\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # avalia a perda\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54570718-a629-4eae-8b7b-7b11a3e3fa7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "João Grilo vamos e arelo,é lhe o padre promete pa rometer quanto  homem. Pobre de jeito \n",
      "senhor \n",
      "amar as pessous bim de fe passe tras no gente ambém vieram. Padre \n",
      "JOÃO GRILO – Ah é um grande joão Grilo me de repeito, Padre João!  \n",
      "PADRE – Não mais nome não ser. Muito pra migo que eu padre nominha para a gente.  \n",
      "JOÃO GRILO – Mue deiro de sconherro é se morrer. E o doutro não todo promarido? Foi e tu a me vinguraça!  \n",
      "CHICÓ – Está aíbaços do tisso e padre essetá ai e queixai o bolvo?  \n",
      "JOÃO GRILO – O cach\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "prompt = 'João Grilo'\n",
    "context = torch.tensor(encode(prompt), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context.unsqueeze(0), max_new_tokens=500)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cf1837-5f7a-4e14-9f6a-872702e959b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "open('exemplo.txt', 'w').write(decode(m.generate(context.unsqueeze(0), max_new_tokens=10000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
